{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa33d262",
   "metadata": {},
   "source": [
    "# Pretraining a RoBERTa-Like Model and Tokenizer\n",
    "Inspired by a project in Rothman's \"Transformers for Naturl Language Processing\", we pretrain KantaiBERT model (a RoBERTa-Like model) on Dostoyevsky's *Crime and Punishment*. We also train a tokenizer with a few more of Dostoyevsky's works (*Poor Folk* and *The Gambler*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0d8d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tokenizers\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e554fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants.\n",
    "data_dir = 'dostoyevsky/'\n",
    "tokenizer_dir = 'KantaiBERT'\n",
    "crime_and_punishment_path = 'dostoyevsky/crime-and-punishment.txt'\n",
    "file_paths = [\n",
    "    str(os.path.join(data_dir, f)) for f in os.listdir(data_dir) \n",
    "    if f.endswith('.txt')\n",
    "]\n",
    "\n",
    "vocab_size = 52000\n",
    "max_tokens = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f7056ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize, train, and save tokenizer.\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=file_paths,\n",
    "    vocab_size=vocab_size,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        '<s>',\n",
    "        '<pad>',\n",
    "        '</s>',\n",
    "        '<unk>',\n",
    "        '<mask>'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create directory.\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir) \n",
    "\n",
    "tokenizer.save_model('KantaiBERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dba3218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'Ä will', 'Ä this', 'Ä sentence', 'Ä be', 'Ä token', 'iz', 'ed', '?']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at an encoded sentence.\n",
    "tokenizer.encode('How will this sentence be tokenized?').tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4ed39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./KantaiBERT/added_tokens.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/tokenizer_config.json. We won't load it.\n",
      "loading file ./KantaiBERT/vocab.json\n",
      "loading file ./KantaiBERT/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n"
     ]
    }
   ],
   "source": [
    "# Configure model and RoBERTa tokenizer.\n",
    "config = transformers.RobertaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    max_position_embeddings=max_tokens+2,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")\n",
    "model = transformers.RobertaForMaskedLM(config=config)\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained(\n",
    "    './KantaiBERT', max_length=max_tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5a4b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 83504416\n",
      "\n",
      "Parameter tensor sizes:\n",
      "\n",
      "0 torch.Size([52000, 768])\n",
      "1 torch.Size([514, 768])\n",
      "2 torch.Size([1, 768])\n",
      "3 torch.Size([768])\n",
      "4 torch.Size([768])\n",
      "5 torch.Size([768, 768])\n",
      "6 torch.Size([768])\n",
      "7 torch.Size([768, 768])\n",
      "8 torch.Size([768])\n",
      "9 torch.Size([768, 768])\n",
      "10 torch.Size([768])\n",
      "11 torch.Size([768, 768])\n",
      "12 torch.Size([768])\n",
      "13 torch.Size([768])\n",
      "14 torch.Size([768])\n",
      "15 torch.Size([3072, 768])\n",
      "16 torch.Size([3072])\n",
      "17 torch.Size([768, 3072])\n",
      "18 torch.Size([768])\n",
      "19 torch.Size([768])\n",
      "20 torch.Size([768])\n",
      "21 torch.Size([768, 768])\n",
      "22 torch.Size([768])\n",
      "23 torch.Size([768, 768])\n",
      "24 torch.Size([768])\n",
      "25 torch.Size([768, 768])\n",
      "26 torch.Size([768])\n",
      "27 torch.Size([768, 768])\n",
      "28 torch.Size([768])\n",
      "29 torch.Size([768])\n",
      "30 torch.Size([768])\n",
      "31 torch.Size([3072, 768])\n",
      "32 torch.Size([3072])\n",
      "33 torch.Size([768, 3072])\n",
      "34 torch.Size([768])\n",
      "35 torch.Size([768])\n",
      "36 torch.Size([768])\n",
      "37 torch.Size([768, 768])\n",
      "38 torch.Size([768])\n",
      "39 torch.Size([768, 768])\n",
      "40 torch.Size([768])\n",
      "41 torch.Size([768, 768])\n",
      "42 torch.Size([768])\n",
      "43 torch.Size([768, 768])\n",
      "44 torch.Size([768])\n",
      "45 torch.Size([768])\n",
      "46 torch.Size([768])\n",
      "47 torch.Size([3072, 768])\n",
      "48 torch.Size([3072])\n",
      "49 torch.Size([768, 3072])\n",
      "50 torch.Size([768])\n",
      "51 torch.Size([768])\n",
      "52 torch.Size([768])\n",
      "53 torch.Size([768, 768])\n",
      "54 torch.Size([768])\n",
      "55 torch.Size([768, 768])\n",
      "56 torch.Size([768])\n",
      "57 torch.Size([768, 768])\n",
      "58 torch.Size([768])\n",
      "59 torch.Size([768, 768])\n",
      "60 torch.Size([768])\n",
      "61 torch.Size([768])\n",
      "62 torch.Size([768])\n",
      "63 torch.Size([3072, 768])\n",
      "64 torch.Size([3072])\n",
      "65 torch.Size([768, 3072])\n",
      "66 torch.Size([768])\n",
      "67 torch.Size([768])\n",
      "68 torch.Size([768])\n",
      "69 torch.Size([768, 768])\n",
      "70 torch.Size([768])\n",
      "71 torch.Size([768, 768])\n",
      "72 torch.Size([768])\n",
      "73 torch.Size([768, 768])\n",
      "74 torch.Size([768])\n",
      "75 torch.Size([768, 768])\n",
      "76 torch.Size([768])\n",
      "77 torch.Size([768])\n",
      "78 torch.Size([768])\n",
      "79 torch.Size([3072, 768])\n",
      "80 torch.Size([3072])\n",
      "81 torch.Size([768, 3072])\n",
      "82 torch.Size([768])\n",
      "83 torch.Size([768])\n",
      "84 torch.Size([768])\n",
      "85 torch.Size([768, 768])\n",
      "86 torch.Size([768])\n",
      "87 torch.Size([768, 768])\n",
      "88 torch.Size([768])\n",
      "89 torch.Size([768, 768])\n",
      "90 torch.Size([768])\n",
      "91 torch.Size([768, 768])\n",
      "92 torch.Size([768])\n",
      "93 torch.Size([768])\n",
      "94 torch.Size([768])\n",
      "95 torch.Size([3072, 768])\n",
      "96 torch.Size([3072])\n",
      "97 torch.Size([768, 3072])\n",
      "98 torch.Size([768])\n",
      "99 torch.Size([768])\n",
      "100 torch.Size([768])\n",
      "101 torch.Size([52000])\n",
      "102 torch.Size([768, 768])\n",
      "103 torch.Size([768])\n",
      "104 torch.Size([768])\n",
      "105 torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# Check the number of parameters.\n",
    "print(f'Number of parameters: {model.num_parameters()}\\n')\n",
    "\n",
    "matrices = list(model.parameters())\n",
    "\n",
    "print('Parameter tensor sizes:\\n')\n",
    "for ind, params in enumerate(matrices):\n",
    "    print(ind, params.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60e78f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at dostoyevsky/crime-and-punishment.txt\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and collator.\n",
    "dataset = transformers.LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=crime_and_punishment_path,\n",
    "    block_size=128\n",
    ")\n",
    "collator = transformers.DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True, mlm_probability=0.15\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17b00cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    output_dir='./KantaiBERT',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0478780b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 17909\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/280 07:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./KantaiBERT\n",
      "Configuration saved in ./KantaiBERT/config.json\n",
      "Model weights saved in ./KantaiBERT/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model('./KantaiBERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a4d0e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading weights file ./KantaiBERT/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./KantaiBERT.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "Didn't find file ./KantaiBERT/tokenizer.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/added_tokens.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/special_tokens_map.json. We won't load it.\n",
      "Didn't find file ./KantaiBERT/tokenizer_config.json. We won't load it.\n",
      "loading file ./KantaiBERT/vocab.json\n",
      "loading file ./KantaiBERT/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n",
      "loading configuration file ./KantaiBERT/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./KantaiBERT\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 52000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform mask filling task with our pretrained model.\n",
    "fill_mask = transformers.pipeline(\n",
    "    'fill-mask',\n",
    "    model='./KantaiBERT',\n",
    "    tokenizer='./KantaiBERT'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f117765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:\n",
      "    May I venture, honoured sir, to engage you in polite conversation?\n",
      "Masked:\n",
      "    May I venture, honoured sir, to engage you in polite <mask>?\n",
      "\n",
      "Prediction: \n",
      "\tMay I venture, honoured sir, to engage you in polite,?\n"
     ]
    }
   ],
   "source": [
    "pred = fill_mask('May I venture, honoured sir, to engage you in polite <mask>?')\n",
    "\n",
    "print(\"\"\"\n",
    "Original:\n",
    "    May I venture, honoured sir, to engage you in polite conversation?\n",
    "Masked:\n",
    "    May I venture, honoured sir, to engage you in polite <mask>?\n",
    "\"\"\")\n",
    "print(f'Prediction: \\n\\t{pred[0][\"sequence\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af618974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original:\n",
      "    His eyes shone with feverish brilliance.\n",
      "Masked:\n",
      "    His eyes shone with feverish <mask>.\n",
      "\n",
      "Prediction: \n",
      "\tHis eyes shone with feverish..\n"
     ]
    }
   ],
   "source": [
    "pred = fill_mask('His eyes shone with feverish <mask>.')\n",
    "\n",
    "print(\"\"\"\n",
    "Original:\n",
    "    His eyes shone with feverish brilliance.\n",
    "Masked:\n",
    "    His eyes shone with feverish <mask>.\n",
    "\"\"\")\n",
    "print(f'Prediction: \\n\\t{pred[0][\"sequence\"]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
