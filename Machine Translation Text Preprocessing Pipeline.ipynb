{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db48b9b2",
   "metadata": {},
   "source": [
    "# Machine Translation Text Preprocessing Pipeline\n",
    "A preprocessing pipeline for the European Parliament Proceedings Parallel Corpus 2011 English-to-Portuguese dataset, functionalized to preprocess general machine translation datasets for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc09ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc45c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions.\n",
    "def load_raw_text(file_name: str):\n",
    "    \"\"\" Load text file and return as list of sentences.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = text.strip().split('\\n')\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def load_clean_text(file_name: str):\n",
    "    \"\"\" Load preprocessed text into memory.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'r') as f:\n",
    "        text = pickle.load(f)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def save_clean_text(sentences: list, file_name: str):\n",
    "    \"\"\" Save subset of sentences to file.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "\n",
    "def preprocess_lines(lines: list):\n",
    "    \"\"\" Clean text line by line.\n",
    "    Remove non-printable characters and numbers, return cleaned strings.\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    # Regex printable character filter.\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    # Translation table that removes punctuation.\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for line in lines:\n",
    "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore') # Normalize.\n",
    "        line = line.decode('UTF-8')\n",
    "        line = line.split() # Split on whitespace.\n",
    "        line = [word.lower() for word in line]\n",
    "        line = [word.translate(table) for word in line] # Remove punctuation.\n",
    "        line = [re_print.sub('', word) for word in line] # Remove non-printable characters.\n",
    "        line = [word for word in line if word.isalpha()] # Remove non-alphabetic characters (numbers).\n",
    "        \n",
    "        cleaned_lines.append(' '.join(line))\n",
    "        \n",
    "    return cleaned_lines\n",
    "\n",
    "def to_vocab(lines: list):\n",
    "    \"\"\" Create vocabulary from list of tokens, return as a Counter.\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        vocab.update(tokens)\n",
    "        \n",
    "    return vocab\n",
    "\n",
    "def trim_vocab(vocab: Counter, min_occurence: int):\n",
    "    \"\"\" Trims vocabulary with a given threshold and returns as a set.\n",
    "    Minimumn occurence is inclusive.\n",
    "    \"\"\"\n",
    "    tokens = [t for t, count_ in vocab.items() if count_ >= min_occurence]\n",
    "    \n",
    "    return set(tokens)\n",
    "\n",
    "def encode_oov(lines: list, vocab: set, fill_token: str='unk'):\n",
    "    \"\"\" Replace out of vocabulary words with given token.\n",
    "    \"\"\"\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        new_tokens = []\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append(token) # Do nothing for tokens in vocab.\n",
    "            else:\n",
    "                new_tokens.append(fill_token) # Replace unknown tokens with 'fill_token'.\n",
    "                \n",
    "        new_lines.append(' '.join(new_tokens))\n",
    "        \n",
    "    return new_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0790f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data preprocessing function.\n",
    "def clean_and_save_file(file_name: str, save_name: str):\n",
    "    \"\"\" Load raw text file, preprocess, and save to desired location.\n",
    "    \"\"\"\n",
    "    sentences = load_raw_text(file_name)\n",
    "    lines = preprocess_lines(sentences)\n",
    "    \n",
    "    full_vocab = to_vocab(lines)\n",
    "    vocab = trim_vocab(full_vocab, 5)\n",
    "\n",
    "    encoded_lines = encode_oov(lines, vocab)\n",
    "    save_clean_text(encoded_lines, save_name)\n",
    "    \n",
    "    return encoded_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e77345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and re-save data.\n",
    "english = clean_and_save_file('data/europarl-v7.pt-en.en', 'data/english.pkl')\n",
    "portuguese = clean_and_save_file('data/europarl-v7.pt-en.pt', 'data/portuguese.pkl')\n",
    "\n",
    "# english = load_clean_sentences('data/english.pkl')\n",
    "# portuguese = load_clean_sentences('data/portuguese.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e56543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in Portuguese: reinicio da sessao\n",
      "Sentence in English: resumption of the session\n",
      "\n",
      "Sentence in Portuguese: declaro reaberta a sessao do parlamento europeu que tinha sido interrompida na sextafeira de dezembro ultimo e renovo todos os meus votos esperando que tenham tido boas ferias\n",
      "Sentence in English: i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
      "\n",
      "Sentence in Portuguese: como puderam constatar o grande bug do ano nao aconteceu em contrapartida os cidadaos de alguns dos nossos paises foram vitimas de catastrofes naturais verdadeiramente terriveis\n",
      "Sentence in English: although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare sentences.\n",
    "for i in range(3):\n",
    "    print(f'Sentence in Portuguese: {portuguese[i]}')\n",
    "    print(f'Sentence in English: {english[i]}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trax]",
   "language": "python",
   "name": "conda-env-trax-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
