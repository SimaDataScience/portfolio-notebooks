{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f274a849",
   "metadata": {
    "id": "f274a849"
   },
   "source": [
    "# Transfer Learning and the Future of Machine Learning\n",
    "In my opinion, what is by far the most exciting trend in machine learning today is its democratization.\n",
    "\n",
    "Thanks to the incredible intellectual and computational investments of researchers at places like Google being released to the public, anybody in the world is able to acheive incredible results on many deep learning tasks with very little time or computational cost. Not only can these models be retrained with large datasets, but more importantly they can also be used to get impressive results with few-shot and even zero-shot learning.\n",
    "\n",
    "Let's get a qualitative understanding of just how effective transfer learning can be by first training a simple LSTM to generate text without any pretraining, and then utilizing a pretrained network to generate text from the same source. Our training data will be from the first installment of the Twilight series. We'll be using OpenAI's state of the art GPT-3 as our pretrained network. With an incredible 175 billion parameters trained on over 45 TB of text data, this transformer model is able to produce state of the art results on many NLP tasks, and can be interacted with using a simple API.\n",
    "\n",
    "It's worth mentioning that this won't exactly be a fair fight, for more reasons than just parameter count. I'm going to be preparing the data differently for each model, and the results will therefore have some inherent differences. I'll discuss the implications of this, and why the adaptability of pretrained networks is such a big deal, later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12f4b3",
   "metadata": {
    "id": "6f12f4b3"
   },
   "source": [
    "## Simple LSTM\n",
    "We'll first create a simple LSTM model to generate text.\n",
    "\n",
    "We will create our training data by splitting our text into 40 character sequences, and predicting the 41st. This is a fairly simple method, but it will allow us to get a large number of training samples from a relatively short book. Every tenth epoch we will print sample predictions from the current network using several different temperatures and seeds. This will allow us to watch as our network learns the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "Zc3b646U1NMX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1649038067459,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "Zc3b646U1NMX",
    "outputId": "d6d53a12-0c5b-4176-dbd8-f2812d5209e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using Google CoLab\n"
     ]
    }
   ],
   "source": [
    "# Colab setup.\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "    COLAB = True\n",
    "    print('Note: using Google CoLab')\n",
    "except:\n",
    "    print('Note: not using Google CoLab')\n",
    "    COLAB = False\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5k98blya1B1B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1649038068591,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "5k98blya1B1B",
    "outputId": "beb9bf77-e361-410b-dfeb-939f0f3799ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Mount drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "t9opOyPu1EVa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1649038068591,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "t9opOyPu1EVa",
    "outputId": "2017996d-7ff3-43d2-8fc2-80621f605619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Project Directories/Portfolio/twilight\n"
     ]
    }
   ],
   "source": [
    "# Set working directory.\n",
    "%cd '/content/drive/My Drive/Project Directories/Portfolio/twilight/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "vj_1KTAL1Hlf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1649038068894,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "vj_1KTAL1Hlf",
    "outputId": "1283b864-bda5-4bec-ede2-5b5233d3e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-93b5fcf7-321b-8d34-0f69-15b63cf66ec8)\n"
     ]
    }
   ],
   "source": [
    "# Display GPU type.\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba7a1f0c",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1649038068894,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "ba7a1f0c"
   },
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b25309ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1649038068895,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "b25309ae",
    "outputId": "a5c8604a-5df8-420d-ed19-468531ee390b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My mother drove me to the airport with the windows rolled down. It was seventy-five degrees in \n",
      "Phoenix, the sky a perfect, cloudless blue. I was wearing my favorite shirt — sleeveless, white eyelet \n",
      "lace; I was wearing it as a farewell gesture. My carry-on item was a parka. \n",
      "\n",
      "In the Olympic Peninsula of northwest Washington State, a small town named Forks exists under a \n",
      "near-constant cover of clouds. It rains on this inconsequential town more than any other place in the \n",
      "United States of America. It was from this town and its gloomy, omnipresent shade that my mother \n",
      "escaped with me when I was only a few months old. It was in this town that I'd been compelled to spend \n",
      "a month every summer until I was fourteen. That was the year I finally put my foot down; these past three \n",
      "summers, my dad, Charlie, vacationed with me in California for two weeks instead. \n",
      "\n",
      "It was to Forks that I now exiled myself — an action that I took with great horror. I detested Forks. \n",
      "\n",
      "I loved Phoenix. I loved \n"
     ]
    }
   ],
   "source": [
    "# Load raw text.\n",
    "with open('twilight1.txt') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Show first 1000 characters.\n",
    "print(raw_text[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2abd5d52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1649038069047,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "2abd5d52",
    "outputId": "dccf90aa-a8e8-48e5-b35e-d920b7f4e429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 665126\n",
      "Total Characters: 50\n"
     ]
    }
   ],
   "source": [
    "# Lowercase text and filter characters.\n",
    "processed_text = raw_text.lower()\n",
    "processed_text = re.sub(r'[^\\x00-\\x7f]',r'', processed_text) \n",
    "\n",
    "# Print total length and number of unique characters.\n",
    "print('Total Length:', len(processed_text))\n",
    "characters = sorted(list(set(processed_text)))\n",
    "\n",
    "print('Total Characters:', len(characters))\n",
    "char_indices = dict((c, i) for i, c in enumerate(characters))\n",
    "indices_char = dict((i, c) for i, c in enumerate(characters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8580bf19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1649038069336,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "8580bf19",
    "outputId": "03b2167f-aa16-4686-87e6-69a346344b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sequences: 221696\n"
     ]
    }
   ],
   "source": [
    "# Slice the data into 40 character overlapping sections.\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(processed_text) - maxlen, step):\n",
    "    sentences.append(processed_text[i: i + maxlen])\n",
    "    next_chars.append(processed_text[i + maxlen])\n",
    "print('Number of Sequences:', len(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eeaefe0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3147,
     "status": "ok",
     "timestamp": 1649038072481,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "5eeaefe0",
    "outputId": "1aa7a556-c0c9-468f-894f-271d120415bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Vectorize character sequences.\n",
    "x = np.zeros((len(sentences), maxlen, len(characters)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(characters)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2e36891",
   "metadata": {
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1649038072629,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "a2e36891"
   },
   "outputs": [],
   "source": [
    "# Build a simple single LSTM.\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(characters))))\n",
    "model.add(Dense(len(characters), activation='softmax'))\n",
    "\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ea1ec38",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649038072630,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "9ea1ec38"
   },
   "outputs": [],
   "source": [
    "# Helper function to sample an index from a probability array.\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03a574c9",
   "metadata": {
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1649038072779,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "03a574c9"
   },
   "outputs": [],
   "source": [
    "# Function taken from Jeff Heaton: https://github.com/jeffheaton\n",
    "def on_epoch_end(epoch, _):\n",
    "    if epoch % 10 == 0:\n",
    "        # Function invoked at end of each epoch. Prints generated text.\n",
    "        print(\"******************************************************\")\n",
    "        print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "        start_index = random.randint(0, len(processed_text) - maxlen - 1)\n",
    "        for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "            print('----- temperature:', temperature)\n",
    "\n",
    "            generated = ''\n",
    "            sentence = processed_text[start_index: start_index + maxlen]\n",
    "            generated += sentence\n",
    "            print('----- Generating with seed: \"' + sentence + '\"')\n",
    "            sys.stdout.write(generated)\n",
    "\n",
    "            for i in range(400):\n",
    "                x_pred = np.zeros((1, maxlen, len(characters)))\n",
    "                for t, char in enumerate(sentence):\n",
    "                    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "                preds = model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = sample(preds, temperature)\n",
    "                next_char = indices_char[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence[1:] + next_char\n",
    "\n",
    "                sys.stdout.write(next_char)\n",
    "                sys.stdout.flush()\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13fae834",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1030115,
     "status": "ok",
     "timestamp": 1649039632835,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "13fae834",
    "outputId": "f708fc9d-91c5-429c-aaa9-fcf0d7e5234d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/61\n",
      "1723/1732 [============================>.] - ETA: 0s - loss: 1.1751******************************************************\n",
      "----- Generating text after Epoch: 0\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"cross the sky, causing the sea to darken\"\n",
      "cross the sky, causing the sea to darken to the room and the stepper of the stagger of the stairs and the still the stairs were still the face that i was a few seconds and the still the stairs were so make the stairs and the room with the the was still and the scent of the dark of the stairs and the stairs were still answering and the still \n",
      "with the stairs and then i was too \n",
      "staring at the stairs and the stairs were something and then\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"cross the sky, causing the sea to darken\"\n",
      "cross the sky, causing the sea to darken to break at me when i was a smotiously. \n",
      "\n",
      "\"alice had to be all the door.\" \n",
      "\n",
      "\"what do you could hear scan away from the day. i tried to be around of \n",
      "his eyes over the fall of my hand to see the door. \n",
      "\n",
      "\"what you meant me bellate-dese of the side of the steering toward the way to scale it out of the fairs, when he said. \n",
      "\n",
      "\"we were alice.\" \n",
      "\n",
      "\"what are you think a second?\" i stared at me for his sho\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"cross the sky, causing the sea to darken\"\n",
      "cross the sky, causing the sea to darken that i wanted to be \n",
      "to pourt agreen to his low \n",
      "an ifficiling sitgeres "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from him not to some prover and helfqug?\" i finally was friends with definitions workied magrape, all this was mornes it \n",
      "for the house day, wondering \n",
      "back. \n",
      "\n",
      "\"how aid calm the shirror indeds?\" i asked, making at repectate five. \n",
      "\n",
      "\"we could tell that. , i never turned to even pafet. that i can't remember you boulf.\" \n",
      "\n",
      "his po\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"cross the sky, causing the sea to darken\"\n",
      "cross the sky, causing the sea to darken, butting me of well, dismsoutting from us about bothering through every the stileing around then i would fee something ammmete in tyllly, bapercigity. it all ext. putitly was old voice up aware hee safe. \n",
      "\n",
      "my bat with debilly grasal like him. his inretriends inrocking. i \n",
      "couldn't be might hfglial her free ben a'd sofmd stoder, extrising. his jace esca. \n",
      "\n",
      "gettings if answer \n",
      "calmake thlick sawaye\n",
      "1732/1732 [==============================] - 78s 45ms/step - loss: 1.1751\n",
      "Epoch 2/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1721\n",
      "Epoch 3/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1704\n",
      "Epoch 4/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1672\n",
      "Epoch 5/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1664\n",
      "Epoch 6/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1660\n",
      "Epoch 7/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1620\n",
      "Epoch 8/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1614\n",
      "Epoch 9/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1596\n",
      "Epoch 10/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1578\n",
      "Epoch 11/61\n",
      "1730/1732 [============================>.] - ETA: 0s - loss: 1.1559******************************************************\n",
      "----- Generating text after Epoch: 10\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"ear you won't leave me?\" i whispered. i \"\n",
      "ear you won't leave me?\" i whispered. i shuddered. \n",
      "\n",
      "\"i was still and i was still the sun. i was still and i was still so faster the clouds and answered to his stand of the stairs and the stairs were the stairs and then i was still so when the walls were some shirt, and i was still handly. \n",
      "\n",
      "\"i was still the stairs and then i was still so plases to my tease and i was still a surning to be alone. i stared at me at the stoppital partness.\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"ear you won't leave me?\" i whispered. i \"\n",
      "ear you won't leave me?\" i whispered. i smiled. \"i said you all the doors that you realize  the next soon, i wanted to get on the thing with already. \n",
      "\n",
      "\"we're the first that alone.\" \n",
      "\n",
      "\"i asked anywhere that i was right on a \n",
      "light was that he was still so the seat cullen was more than i was some that just as they should be so a minute to answer. he was still the doors at the thing constant down. \n",
      "\n",
      "\"you don't you think i was did you an r\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"ear you won't leave me?\" i whispered. i \"\n",
      "ear you won't leave me?\" i whispered. i ble sound lightly dream. \n",
      "\n",
      "\"lunges onther book cried.\" his eyes were green with the charious slowly. he madaked out the drive in his reflew b\n",
      "forp, except a little aran them. and emmett provens that they three i didn't have a like escape to minc?bfur of him, it. \n",
      "\n",
      "\"\n",
      "\n",
      "\"i hunt many one fire now, put was disgoret. madever. aksting him pushing as light hate and exaggled a bleening cabbered \n",
      "boupin mom\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"ear you won't leave me?\" i whispered. i \"\n",
      "ear you won't leave me?\" i whispered. i canted to three. his quit inry work, ie \n",
      "bence several coasifle. then deskly defitied and then ssweries had sittly rang, armile there laured the \n",
      "difficultica from enough one w all race our open. my zzach on the same cool, ith wap, give flash, songed outside dies simion. as they ing begging at anice. it was too concentratst of the neve spork, stupidanly was like his heess. \n",
      "\n",
      "he slipped, \n",
      "slamm. th\n",
      "1732/1732 [==============================] - 78s 45ms/step - loss: 1.1559\n",
      "Epoch 12/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1559\n",
      "Epoch 13/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1535\n",
      "Epoch 14/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1527\n",
      "Epoch 15/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1507\n",
      "Epoch 16/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1510\n",
      "Epoch 17/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1496\n",
      "Epoch 18/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1482\n",
      "Epoch 19/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1477\n",
      "Epoch 20/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1462\n",
      "Epoch 21/61\n",
      "1725/1732 [============================>.] - ETA: 0s - loss: 1.1473******************************************************\n",
      "----- Generating text after Epoch: 20\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\"\n",
      "?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\n",
      "\"i'm not in the stairs about the stairs to the stairs away. \n",
      "\n",
      "\"i wonder that he was stunned that we were so the stairs the stairs about the stairs at the teach on the continuss. \n",
      "\n",
      "\"i don't know it was in the stairs about the \n",
      "dance of the stairs away. \n",
      "\n",
      "\"what i was something i was some way to the word. \n",
      "\n",
      "\"what you were something to the same that all the class as i was standing to the stairs away.\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\"\n",
      "?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\n",
      "\"the gold answay?\" i asked. \n",
      "\n",
      "\"not come. he'll see the \n",
      "day. \n",
      "i didn't gone him at the house and held the hunter of the desk that i was still to look at him to me. \n",
      "\n",
      "\"he said you were back on the friend and bright to the back of the stairs to his back to the room, and it would be for a lot s?\" \n",
      "\n",
      "we glanced looking forward against the back of my stare of his grin to see his hand and walked to the \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\"\n",
      "?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\n",
      "he snickered charlie's fordatal \n",
      "side of the famil settle. \n",
      "\n",
      "\"you'have to day the \n",
      "jeaus hand for a few cint of shown. \n",
      "\n",
      "i don't look the vere part. \n",
      "\n",
      "he turned me. \n",
      "\n",
      "\"you're nice there you think a molder to bring. my chese, he fell antelced untimically, he over a whisper, i could hear him in the centery. legher bother. \n",
      "\n",
      "\"no, som. led the proph fack and clenched grinning as i went word. \n",
      "\"the mo\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "\"\n",
      "?\" \n",
      "\n",
      "\"he'll be here in a few minutes.\" \n",
      "i glanced again. as the chdeage from on. \n",
      "\n",
      "mike's under needs to soon. resolu. fier. he fromatherplied langray hoping him as i all i had perceened the surn, \n",
      "con littled tired hand. i'd felt mike's heart. he closed bared towa. \"where  i can call you to heremon chaeh, are the jessact?\" but in the eablextwibl. his carpint unlocked ahst. there that \n",
      "it. i shook my hand here fill the swife. i expeced \n",
      "1732/1732 [==============================] - 78s 45ms/step - loss: 1.1472\n",
      "Epoch 22/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 1.1682\n",
      "Epoch 23/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 4.8123\n",
      "Epoch 24/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 9.3442\n",
      "Epoch 25/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 7.9773\n",
      "Epoch 26/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 6.4971\n",
      "Epoch 27/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 5.9512\n",
      "Epoch 28/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 5.7226\n",
      "Epoch 29/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 5.3125\n",
      "Epoch 30/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 4.7977\n",
      "Epoch 31/61\n",
      "1724/1732 [============================>.] - ETA: 0s - loss: 4.5171******************************************************\n",
      "----- Generating text after Epoch: 30\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"ng the \n",
      "loneliest places, loathing himse\"\n",
      "ng the \n",
      "loneliest places, loathing himse  te as t\n",
      "d  t  to ed ta  hit \n",
      " jaa \n",
      "w t te te e toe whver  i  saede i   t my tin j   tete  sher m thae  h  tinw th that t as t \n",
      " then t   the  te \n",
      "\n",
      "\n",
      "\n",
      " tat \n",
      "e t   th?  h woug qa eee  i t t  ngre w ten  hn   i t  \n",
      "t t \n",
      " as  \n",
      "\n",
      "hi -t t et  w w ba the aoja e \n",
      "t   ate n ae b \n",
      "\"thr  d hej   t  ana  tin  e \n",
      "\n",
      "e  a a  tan \n",
      "i fo i   at wace tb  \n",
      "wo the i   hn ta a t tot the   ae \"ta ane   he  tte arhe ot o \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"ng the \n",
      "loneliest places, loathing himse\"\n",
      "ng the \n",
      "loneliest places, loathing himse  honnt haekt   t  w o e\"  ape \"d wat ee sa tv teth q  t inle tobetty le\"ee\n",
      "vein itoe e coe t st   ftamin a \n",
      "\n",
      " \"alae  t c g aitm  muapd sta maned ir sfeg   w tew tet y  so dr tea eaoo m ineet e stede ghu  \n",
      "ch  tmn  \" n sq i  ta t the  h ig  th eti te  e w t  \n",
      "h coa cy e red \n",
      " \n",
      " \n",
      " wrejve  h en  td s auce  e hhal  eange loe i heti bedtr w he tate e  rana\n",
      "    astd taa lhe so i tibn\n",
      "e  oe wet  awts  a\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"ng the \n",
      "loneliest places, loathing himse\"\n",
      "ng the \n",
      "loneliest places, loathing himseloe \n",
      " waor  inlsss.agira \n",
      "\n",
      "e,i e ap,  hstateukg  isbirmh moekne whah inksm \"af\"igipt,inzt emar   wo\",l md'ais, ninghebmehemt  tyohrea dnradsa.la ruptinht r ogsl ll\n",
      "t wuerdidleetolarrgdtwoytbckolato gee,thnho we \n",
      "y.h\n",
      "oringae l\n",
      "ilei ce  be\n",
      "ndtedgf rktreime \n",
      "ke tinietk islygwhepedpunaitgelet.ontwmh, otnrin wheoobighotwhtb eddnthh n yonewryirela\n",
      "it te toaunedrmyes,t ndyrleghm, tkn lheesu\n",
      "le th. nridye\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"ng the \n",
      "loneliest places, loathing himse\"\n",
      "ng the \n",
      "loneliest places, loathing himselo peeoad.gd atl;uskelelatd,tirnfun aewis.hles s ait comeas tengveeth\" s gre' wis rd ati.o.\" dd ntruy odk re nafb spoltre s\n",
      "\"yotdmi olitehddr\n",
      "amd porkkorg1rso leetr -npa saly htleyem doeh.\" bftyn wous fitwsfr salis wa,w.raaem.abtr i loc iytooetiei tnkd,tafo p bta.. t eslw,c sasi\".whhtik nid iny ho al,dtws.ik whsne ,ah.one\n",
      "dig nnr?i asenyteab vaekruesuan mood.s\n",
      "iceafosteaodilto s\"hi dakisyhiso.dhe\"\n",
      "1732/1732 [==============================] - 77s 45ms/step - loss: 4.5160\n",
      "Epoch 32/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 4.1804\n",
      "Epoch 33/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 4.0004\n",
      "Epoch 34/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.9096\n",
      "Epoch 35/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.7530\n",
      "Epoch 36/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.5690\n",
      "Epoch 37/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.4303\n",
      "Epoch 38/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.3817\n",
      "Epoch 39/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.3156\n",
      "Epoch 40/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.2038\n",
      "Epoch 41/61\n",
      "1731/1732 [============================>.] - ETA: 0s - loss: 3.1577******************************************************\n",
      "----- Generating text after Epoch: 40\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"d my face back. i opened my eyes and saw\"\n",
      "d my face back. i opened my eyes and sawe e sanh   th \n",
      "het t teanth en e   t t it  \n",
      "e teuttle e the en  te  ee   \n",
      "o t t    te i t e hee e tea tn \n",
      "th  he i t t  th te to e an t t  e tei s t to i  e  t t t  h   h t  a  th t e h a  tin  t e t  \n",
      " in   hh th she \n",
      "\n",
      "s te a i e thet i i a t  h   to e  i  \n",
      " \" to nd  t in  en s h i the  \n",
      " e i t  tt a n i h t  ias   \n",
      "n inad ei h tr eno th isee \n",
      " \n",
      " oe \n",
      "\n",
      " i t  a\" e an t   ne \n",
      " e e  \n",
      " tha te eni en t\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"d my face back. i opened my eyes and saw\"\n",
      "d my face back. i opened my eyes and saw leir \n",
      "nersut na r ii  griee\n",
      " w hoona  \" h ee \n",
      " one i oytice sout.tos sras f aus  unie nnn o  ai s le ne i eni t t aymt aa   he it  th \n",
      " he ritd egti   ss ttl th rhen \n",
      " te  \n",
      "e i ou ne denh eaoe i etet tya  inenhi tee te thhe ienaise t\" se\" adpe  e  oe \n",
      "u'h u  \n",
      "ryisere  tote aem hnuse \n",
      "tere noe or nasnte   n hsnawer iee  cn  he sot \n",
      "l\"noyohe   i  ihe  e ne an l  ne \n",
      "\"toh ao  wo aota we  il ha oin t\n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"d my face back. i opened my eyes and saw\"\n",
      "d my face back. i opened my eyes and sawadcie\n",
      "nnddr. sodud? aenr\"rntn ichpsyese \n",
      " memeoed\n",
      "taidesititore,i.rl. unolehay. \"e tsyik. ihitlk omrrd v\n",
      "emen db edory.et  hshmesnrruter rdae oesit,eodrrid \"emra' t e cate cna a\" cden dt uvd niroege,tie,m tkes  \n",
      "s bd y fe canompe ti ingt  bghe aoned p dakona hancided ongclw ,l stid inof \n",
      "heoe nyed ee.p ki tli.  ae \"you luv me eie'. h apayeoshg een h\"hct cs ifcadieed \"fsg rctliunatgoa enehen tksva.\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"d my face back. i opened my eyes and saw\"\n",
      "d my face back. i opened my eyes and sawe ,sewegilythahinepvtakhrs hthyrie gucea,euacrt  kltmy t, tres tmh.dd poterm.\n",
      "t lh ltnshe ctbdaetn.\n",
      "bls fangyerod ao g.\n",
      "odttna.yflgcufive e i,dlmenrdleyegep   bsveuewelgps,\n",
      "whfina\n",
      "neto.\n",
      "ntag unutdmjhe,.ury khht skstyoinue\"k  shwqsr\n",
      "sy cnaeenl m\"\"tsned tant. lesleya  'v ooe o w hleerno,etlurtooofc ppdc.o.rlue ihrgirujripy n nmae io,i  ttthonhe   dn ia k\n",
      "tc bqc\"aht  atcfgenheg b lsed i,mnga chnku es\n",
      "1732/1732 [==============================] - 78s 45ms/step - loss: 3.1575\n",
      "Epoch 42/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.1422\n",
      "Epoch 43/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.1350\n",
      "Epoch 44/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.1210\n",
      "Epoch 45/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.1190\n",
      "Epoch 46/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0957\n",
      "Epoch 47/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0943\n",
      "Epoch 48/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0877\n",
      "Epoch 49/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0762\n",
      "Epoch 50/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0626\n",
      "Epoch 51/61\n",
      "1729/1732 [============================>.] - ETA: 0s - loss: 3.0483******************************************************\n",
      "----- Generating text after Epoch: 50\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"\n",
      "was  still in my duffle bag under the b\"\n",
      "\n",
      "was  still in my duffle bag under the b\n",
      "  st e e oto eh  h  e  ae \n",
      " h nereunt  e  e   h s s a tia e   a   ie te tha t  t t a   n  ie    a t n e toe  e te te e t  n t   t n  \n",
      "e t    ssa a ax  te hth haooee ea  thn awee eeroie  wete te e n  er sai au i  do w  t  \n",
      "eea the t  n ant   tee ae thte te ee ten \n",
      "  th ta we t t   e e  e wast s  t e we sa   \n",
      "  te  t o iae he   a te tu e   n   te  te    ue    a t e  as  e tond e  \n",
      "   a nn o te t  a\n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"\n",
      "was  still in my duffle bag under the b\"\n",
      "\n",
      "was  still in my duffle bag under the bhner  \n",
      "h\" hon bi b men\"ere tkee geeu hso testhe  \n",
      "i  dea tde, \n",
      "eert\" romutin no ralar.  al  ant   l hic tt \n",
      "l se s lo e t\"t  e t  ghe attie ae\n",
      " dee e ert trh\" tte rhr woirhe d a di lamn rha e \n",
      "\n",
      "at\"ehwnc o yiour itle  sen he anr swed oe m \n",
      "in w e tou  a a e  e a s h n s auenmue \n",
      "th i re i ho hithtie ant   t adt aaaer t telnde t oe a iet   ntart f th hna  e  et  tolene a t nenanium enennf csst\"le y \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"\n",
      "was  still in my duffle bag under the b\"\n",
      "\n",
      "was  still in my duffle bag under the bwnor gunw \n",
      " th opeena w,erat t.rt , rec,loido ad olistte\"cinle waeann edalu oi bh waa.i lcexumita   n d tar,c' f nma tre un beh\n",
      " \n",
      " hxe yew\n",
      "t antule?rhrged bisrosesorgif iiotd tswe i letautcwti'usherehnergroe' ti oourd a steg mohd  tt o s\n",
      "slodkn a.idiiet'cn,eh  ftebe   mwno\"oen  yendee, \n",
      "e ca c uolaet sgxt pte r hst is evhrer,\" rhoidglenrceva beswlnpmtk e oe m iaigayethoa  mt miykb  s ami,s \n",
      "biep y\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"\n",
      "was  still in my duffle bag under the b\"\n",
      "\n",
      "was  still in my duffle bag under the bhehyiara wl  waia \n",
      "pcm mwikibantsi wme,u oeukor oleas?g,ay couhetdhadof us\"ue sckise,eof d ooufdncl mai afekeau\"s m arf.ut.noy ihodas euafnyebel sa\"\"  gghk\"gddoamihoawrloe  sld std s \n",
      "tns\n",
      "ek.bgesydliod s yaayed'da cull'gng he \n",
      "eale.priivire  le,m c.e w ns? har n  entgo, itn  ncw notoiahdok  halmay\"aognedpd ocr tlm enp tauptuik\n",
      "l\n",
      "er 'tma tafjoinyo nhs\"mm,ewmyia \"iroe 'orefedelimoen hhnts'h.hln ere,\n",
      "1732/1732 [==============================] - 78s 45ms/step - loss: 3.0491\n",
      "Epoch 52/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0427\n",
      "Epoch 53/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0347\n",
      "Epoch 54/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0283\n",
      "Epoch 55/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0229\n",
      "Epoch 56/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 3.0063\n",
      "Epoch 57/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 2.9950\n",
      "Epoch 58/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 2.9904\n",
      "Epoch 59/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 2.9817\n",
      "Epoch 60/61\n",
      "1732/1732 [==============================] - 9s 5ms/step - loss: 2.9742\n",
      "Epoch 61/61\n",
      "1727/1732 [============================>.] - ETA: 0s - loss: 2.9638******************************************************\n",
      "----- Generating text after Epoch: 60\n",
      "----- temperature: 0.2\n",
      "----- Generating with seed: \"t which their bodies raced around the fi\"\n",
      "t which their bodies raced around the fie    \n",
      "i \n",
      " i  hate e a t ae i  ho     o   te  t  t  t  o  o     e  te te t a s ton  \n",
      "\n",
      "\n",
      "  ntto \n",
      " t   b\n",
      " t wa  \n",
      " wa thn  \n",
      "\n",
      "t s t   \n",
      "t  he e    o hay \n",
      "\n",
      "\" \n",
      "\n",
      "e ee t o  w ah th   t  \n",
      "at e t a    t     t e     n   aex   h n e t me in   t  toe ti an to a a te  toe   \n",
      " too h o  ts ae    t e   t  t o   te th e e or tto  t  o t  oue itte o  \n",
      "e  het  ht  o   o n te te  \n",
      "aa ne   \n",
      "\"o  an o so the   oo   t  o  t \n",
      "----- temperature: 0.5\n",
      "----- Generating with seed: \"t which their bodies raced around the fi\"\n",
      "t which their bodies raced around the finauy o  td hh o   te s h tnce  lihe. t  sp i aa  s e ide ond t tee  iel  tn oen mn ra tq d aia tn s th e irhn mp tt o   k t s oa \n",
      "\n",
      "\n",
      " h\n",
      "aadwo \" s n an tf   e tmh \n",
      "s te eae ia hla b tas d hr r\" i  i ee  si fe  h ne  se  msir tths tie \n",
      "m man t me  the  iia thne  he  a esen shu a  b t   ee csli moue  at ahe te \n",
      "  \n",
      "o is e b wee it tid o e  t oahee tirse \n",
      "et me hede  \n",
      "  f\n",
      "a i  a oo rahdid st thse   ht  \n",
      "----- temperature: 1.0\n",
      "----- Generating with seed: \"t which their bodies raced around the fi\"\n",
      "t which their bodies raced around the fiop  hiiouen \n",
      "tsd tamulo,n,taei \n",
      "rhamir \n",
      " tittebinrn ct ngin le  py'cw in ihmkh s, \n",
      " har yoedarisus, shtgll\n",
      ".wedarg seriy opr me\"ltallsrehoc ade a\n",
      "edperagl s,  ew kpinnle cit\"k ed drdn ycial eehmoeailge fwtmalwiaeny  \n",
      "\n",
      "st. rgoesingor ht wo   s hi lr,st.hne. \n",
      "taq  ogtaan-ok  itthosl chd moeudsw,s t riii  act tb cd f m d 'mes\". nht\"etestsdcnaetan'i \"e c    he i botitmyo enchestrb  b toiee bhk.r sb he\n",
      "----- temperature: 1.2\n",
      "----- Generating with seed: \"t which their bodies raced around the fi\"\n",
      "t which their bodies raced around the fisyeneu hteyeeyoas\n",
      "o ntmr wearibcyoele oneve ntlaau.ethkr\n",
      "wertthe hgachoked.eardvudeo ie ri keem.oa amdd, ugv,',s?urd socl y, mbnnal meuci ve,iddea mple mr?n inkfklwrcmgw ss na nweeoi iteor\"iunhldys' aieve\"nkns f?wuis ptse ss, m.ed s sno f\n",
      "\"bg  thne w menhplbllnge my\"t tud \n",
      "t' m p,er .th oucinelitnl cmho\n",
      "tlenaire.dmoh i  reees,a h n ke. n pton. hemt tolrsae kacelesmlte  semid.ann crihor  bnd msh 'i\n",
      "1732/1732 [==============================] - 77s 45ms/step - loss: 2.9636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8c520b5950>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=61,\n",
    "          callbacks=[print_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f4cb29",
   "metadata": {
    "id": "51f4cb29"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a6fba",
   "metadata": {
    "id": "e52a6fba"
   },
   "source": [
    "It's incredible to see how much our model was able to learn about the English language so quickly, but let's be honest: nobody would mistake this for human language. Moreover, even though this model is very simple, it still overfits very quickly - the text no longer resembles English after about 20 epochs. This makes sense, since one book just doesn't provide enough data for training an effective model. Next, let's see how we can approach this task with GPT-3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26161881",
   "metadata": {
    "id": "26161881"
   },
   "source": [
    "## GPT-3\n",
    "Next we'll perform the same task with GPT-3. Since GPT-3 is very adaptable we can use the raw data as input, however the version I'm using has a maximum token length of around 4,096. Therefore we can only choose this much of our text to serve as our example. We'll choose an aribtrary section of the book and ask GPT-3 to generate 250 tokens worth of dialogue based on the example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4c238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw text.\n",
    "with open('twilight1.txt') as f:\n",
    "    raw_text = f.read()\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0f371f",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "aborted",
     "timestamp": 1649038597821,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "cf0f371f"
   },
   "outputs": [],
   "source": [
    "import secret\n",
    "\n",
    "def GPT_Generation(prompt, max_tokens, temperature):\n",
    "    # Call API using my key\n",
    "    openai.api_key = secret.API_KEY\n",
    "    \n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        top_p = 1,\n",
    "        max_tokens=max_tokens,\n",
    "        frequency_penalty = 0,\n",
    "        presence_penalty = 0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d302f485",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "aborted",
     "timestamp": 1649038597822,
     "user": {
      "displayName": "Justin Sima",
      "userId": "00504526623099233690"
     },
     "user_tz": 360
    },
    "id": "d302f485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' type, and I immediately distrusted \\nhim. \\n\\n\"Yeah,\" I muttered. \\n\\n\"I\\'m Eric Yorkie,\" he said, holding out his hand. I shook it. \"I live a few houses down from your dad. I \\nknow Charlie, he\\'s a great guy.\" \\n\\n\"Yeah,\" I said again. I was getting really good at this. \\n\\n\"You just moved here from Arizona, right?\" \\n\\n\"Yeah.\" \\n\\n\"Do you know where your next class is?\" \\n\\nI shook my head, no. \\n\\n\"Here, let me show you.\" He pulled my schedule out of my hand. \"You have lunch now, so you don\\'t \\nhave to go anywhere. I\\'ll walk you to your next class after lunch.\" \\n\\n\"That\\'s okay, I can find it.\" I tried to take the schedule back, but he was already walking away, and I \\ndidn\\'t want to make a scene. \\n\\n\"I don\\'t mind,\" he called over his shoulder. \"I have to go this way anyway.\" \\n\\nI sighed and followed him.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_heading = 'Generate dialogue text based on the following example: '\n",
    "prompt = prompt_heading + raw_text[15000:19000]\n",
    "\n",
    "GPT_Generation(prompt=prompt, max_tokens=250, temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d6afa",
   "metadata": {
    "id": "ea5d6afa"
   },
   "source": [
    "Unsurprisngly, the model produced gramatically correct dialogue. What is surprising, however, is how well GPT-3 was able to replicate the tone of the book with exposure to only 4,000 characters (and that includes line breaks!). We were able to far outperform our LSTM with less data, in less time, and with far fewer lines of code. While this particular example of transfer learning is fairly trivial, the applications are seemingly endless. And as more and more of these models are released to the public, the world's most powerful tools in artificial intelligence will increasingly be able to solve everyday problems for anybody with a laptop and some imagination.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twilight LSTM Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
